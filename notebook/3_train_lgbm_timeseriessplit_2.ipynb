{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from itertools import combinations\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "gc.enable()\n",
    "pd.set_option('display.max_columns', 200)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_dir_path = pathlib.Path('../inputs')\n",
    "outputs_dir_path = pathlib.Path('../outputs')\n",
    "if not outputs_dir_path.is_dir():\n",
    "    outputs_dir_path.mkdir()\n",
    "\n",
    "train_df = pd.read_csv(inputs_dir_path. joinpath('train.csv'))\n",
    "train_df.drop(columns=['row_id', 'time_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_dtypes = {\n",
    "    'stock_id': np.int16,\n",
    "    'date_id': np.int16,\n",
    "    'seconds_in_bucket': np.int16,\n",
    "    'imbalance_size': np.float32,\n",
    "    'imbalance_buy_sell_flag': np.int16,\n",
    "    'reference_price': np.float32,\n",
    "    'matched_size': np.float32,\n",
    "    'far_price': np.float32,\n",
    "    'near_price': np.float32,\n",
    "    'bid_price': np.float32,\n",
    "    'bid_size': np.float32,\n",
    "    'ask_price': np.float32,\n",
    "    'ask_size': np.float32,\n",
    "    'wap': np.float32,\n",
    "    'target': np.float32\n",
    "}\n",
    "\n",
    "display(train_df.dtypes)\n",
    "train_df = train_df.astype(cast_dtypes)\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "])\n",
    "\n",
    "# index_wap = (\n",
    "#     train_df\n",
    "#     .groupby(['date_id', 'seconds_in_bucket'])\n",
    "#     .apply(lambda x: weights[x['stock_id']] * x['wap']).sum()\n",
    "# )\n",
    "# index_wap = pd.DataFrame(index_wap, columns=['index_wap'])\n",
    "# train_df = train_df.merge(index_wap, on=['date_id', 'seconds_in_bucket'])\n",
    "\n",
    "weights_df = pd.DataFrame(weights, index=pd.Series(np.arange(200), name='stock_id'), columns=['weights'])\n",
    "train_df = train_df.merge(weights_df, on='stock_id')\n",
    "display(train_df)\n",
    "train_df.eval('index_wap = weights * wap', inplace=True)\n",
    "display(weights_df)\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = pl.from_pandas(df)\n",
    "\n",
    "    new_features0 = [\n",
    "        (pl.col('date_id') % 5).alias('day_of_week'),\n",
    "    ]\n",
    "    new_features1 = [\n",
    "        (pl.col('ask_price') - pl.col('bid_price')).alias('feature1'),\n",
    "        (pl.col('ask_price') - pl.col('reference_price')).alias('feature2'),\n",
    "        (pl.col('bid_price') - pl.col('reference_price')).alias('feature3'),\n",
    "        (pl.col('ask_price') - pl.col('wap')).alias('feature4'),\n",
    "        (pl.col('bid_price') - pl.col('wap')).alias('feature5'),\n",
    "        (pl.col('far_price') - pl.col('near_price')).alias('feature6'),\n",
    "        (pl.col('far_price') - pl.col('reference_price')).alias('feature7'),\n",
    "        (pl.col('near_price') - pl.col('reference_price')).alias('feature8'),\n",
    "        (pl.col('index_wap') - pl.col('reference_price')).alias('feature9'),\n",
    "        (pl.col('index_wap') - pl.col('ask_price')).alias('feature10'),\n",
    "        (pl.col('index_wap') - pl.col('bid_price')).alias('feature11'),\n",
    "        (pl.col('index_wap') - pl.col('far_price')).alias('feature12'),\n",
    "        (pl.col('index_wap') - pl.col('near_price')).alias('feature13'),\n",
    "        (pl.col('index_wap') - pl.col('wap')).alias('feature14'),\n",
    "        (pl.col('ask_size') - pl.col('bid_size')).alias('feature15'),\n",
    "        (pl.col('ask_size') - pl.col('matched_size')).alias('feature16'),\n",
    "        (pl.col('bid_size') - pl.col('matched_size')).alias('feature17'),\n",
    "        (pl.col('imbalance_size') - pl.col('matched_size')).alias('feature18'),\n",
    "        (pl.col('ask_price') + pl.col('bid_price')).alias('feature19'),\n",
    "        (pl.col('far_price') + pl.col('near_price')).alias('feature20'),\n",
    "        (pl.col('ask_size') + pl.col('bid_size')).alias('feature21'),\n",
    "        ((pl.col('ask_price') - pl.col('bid_price')) * ((pl.col('ask_size') - pl.col('bid_size')) / (pl.col('ask_size') + pl.col('bid_size')))).alias('feature22'),\n",
    "        ((pl.col('ask_price') - pl.col('reference_price')) * ((pl.col('ask_size') - pl.col('bid_size')) / (pl.col('ask_size') + pl.col('bid_size')))).alias('feature23'),\n",
    "        ((pl.col('bid_price') - pl.col('reference_price')) * ((pl.col('ask_size') - pl.col('bid_size')) / (pl.col('ask_size') + pl.col('bid_size')))).alias('feature24'),\n",
    "        ((pl.col('ask_price') - pl.col('wap')) * ((pl.col('ask_size') - pl.col('bid_size')) / (pl.col('ask_size') + pl.col('bid_size')))).alias('feature25'),\n",
    "        ((pl.col('bid_price') - pl.col('wap')) * ((pl.col('ask_size') - pl.col('bid_size')) / (pl.col('ask_size') + pl.col('bid_size')))).alias('feature26'),\n",
    "        ((pl.col('ask_price') - pl.col('bid_price')) * ((pl.col('matched_size') - pl.col('imbalance_size')) / (pl.col('matched_size') + pl.col('imbalance_size')))).alias('feature27'),\n",
    "        ((pl.col('ask_price') - pl.col('reference_price')) * ((pl.col('matched_size') - pl.col('imbalance_size')) / (pl.col('matched_size') + pl.col('imbalance_size')))).alias('feature28'),\n",
    "        ((pl.col('bid_price') - pl.col('reference_price')) * ((pl.col('matched_size') - pl.col('imbalance_size')) / (pl.col('matched_size') + pl.col('imbalance_size')))).alias('feature29'),\n",
    "        ((pl.col('ask_price') - pl.col('wap')) * ((pl.col('matched_size') - pl.col('imbalance_size')) / (pl.col('matched_size') + pl.col('imbalance_size')))).alias('feature30'),\n",
    "        ((pl.col('bid_price') - pl.col('wap')) * ((pl.col('matched_size') - pl.col('imbalance_size')) / (pl.col('matched_size') + pl.col('imbalance_size')))).alias('feature31'),\n",
    "\n",
    "        (pl.col('bid_price') / pl.col('ask_price')).alias('feature32'),\n",
    "        (pl.col('reference_price') / pl.col('ask_price')).alias('feature33'),\n",
    "        (pl.col('reference_price') / pl.col('bid_price')).alias('feature34'),\n",
    "        (pl.col('wap') / pl.col('ask_price')).alias('feature35'),\n",
    "        (pl.col('wap') / pl.col('bid_price')).alias('feature36'),\n",
    "        (pl.col('index_wap') / pl.col('ask_price')).alias('feature37'),\n",
    "        (pl.col('index_wap') / pl.col('bid_price')).alias('feature38'),\n",
    "        (pl.col('index_wap') / pl.col('reference_price')).alias('feature39'),\n",
    "        (pl.col('index_wap') / pl.col('wap')).alias('feature40'),\n",
    "\n",
    "        (pl.col('bid_size') / pl.col('ask_size')).alias('feature41'),\n",
    "        (pl.col('ask_size') / pl.col('matched_size')).alias('feature42'),\n",
    "        (pl.col('bid_size') / pl.col('matched_size')).alias('feature43'),\n",
    "        (pl.col('bid_size') / pl.col('imbalance_size')).alias('feature44'),\n",
    "        (pl.col('ask_size') / pl.col('imbalance_size')).alias('feature45'),\n",
    "        (pl.col('matched_size') / pl.col('imbalance_size')).alias('feature46'),\n",
    "    ]\n",
    "\n",
    "    base_features = [\n",
    "        'imbalance_size',\n",
    "        'imbalance_buy_sell_flag',\n",
    "        'matched_size',\n",
    "        'reference_price',\n",
    "        'far_price',\n",
    "        'near_price',\n",
    "        'bid_size',\n",
    "        'bid_price',\n",
    "        'ask_size',\n",
    "        'ask_price',\n",
    "        'wap',\n",
    "        'index_wap',\n",
    "    ]\n",
    "    #base_features += [f'feature{i}' for i in range(1, 47)]\n",
    "    \n",
    "    new_features2 = [\n",
    "        pl.col(base_features).sort_by(['date_id', 'seconds_in_bucket']).pct_change(n=1).over(['stock_id']).name.prefix('pct_change1_'),\n",
    "        pl.col(base_features).sort_by(['date_id', 'seconds_in_bucket']).pct_change(n=6).over(['stock_id']).name.prefix('pct_change6_'),\n",
    "        pl.col(base_features).sort_by(['date_id', 'seconds_in_bucket']).pct_change(n=12).over(['stock_id']).name.prefix('pct_change12_'),\n",
    "    ]\n",
    "\n",
    "    new_features3 = [\n",
    "        ((pl.col(feature) - pl.col(feature).mean()) / pl.col(feature).std())\n",
    "        .over(['date_id', 'seconds_in_bucket'])\n",
    "        .alias(f'standardized_{feature}')\n",
    "        for feature in base_features\n",
    "    ]\n",
    "\n",
    "    replace_inf = [\n",
    "        pl.when(pl.col(feature).is_infinite()).then(None).otherwise(pl.col(feature)).name.keep()\n",
    "        for feature in df.columns\n",
    "    ]\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .with_columns(new_features0)\n",
    "        .with_columns(new_features1)\n",
    "        .with_columns(new_features2)\n",
    "        .with_columns(new_features3)\n",
    "        .with_columns(replace_inf)\n",
    "        .drop(['row_id'])\n",
    "        .to_pandas()\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocess(train_df)\n",
    "train_df = train_df.dropna(subset=['target'])\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train lightgbm models using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        df: pd.DataFrame,\n",
    "        model_params: Dict[str, Any],\n",
    "        outputs_dir: pathlib.Path,\n",
    "        step: int,\n",
    "    ):\n",
    "    \n",
    "    target_columns = ['stock_id', 'date_id', 'seconds_in_bucket', 'target']\n",
    "    feature_columns = [col for col in df.columns if col not in ['date_id', 'target']]\n",
    "    days= np.arange(df['date_id'].min(), df['date_id'].max())\n",
    "    fimps = []\n",
    "    history = {\n",
    "        'train_mae': [],\n",
    "        'valid_mae': [],\n",
    "    }\n",
    "    oofs = []\n",
    "\n",
    "    kfold = TimeSeriesSplit(n_splits=480//step - 1, test_size=step, gap=0)\n",
    "\n",
    "    for k, (train_indices, valid_indices) in enumerate(kfold.split(days)):\n",
    "        train_days = days[train_indices]\n",
    "        valid_days = days[valid_indices]\n",
    "        print(f'fold {k+1}')\n",
    "        print(train_days)\n",
    "        print(valid_days)\n",
    "        \n",
    "        plot_time(days, train_days, valid_days)\n",
    "        \n",
    "        train_X = df.query('date_id in @train_days')[feature_columns]\n",
    "        train_y = df.query('date_id in @train_days')[target_columns]\n",
    "        valid_X = df.query('date_id in @valid_days')[feature_columns]\n",
    "        valid_y = df.query('date_id in @valid_days')[target_columns]\n",
    "        print(f'train_X.shape: {train_X.shape}, train_y.shape: {train_y.shape}')\n",
    "        print(f'valid_X.shape: {valid_X.shape}, valid_y.shape: {valid_y.shape}')\n",
    "\n",
    "        callbacks = [\n",
    "            lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
    "            lgb.log_evaluation(500),\n",
    "        ]\n",
    "        \n",
    "        train_dataset = lgb.Dataset(\n",
    "            train_X,\n",
    "            train_y['target'],\n",
    "            #categorical_feature=['imbalance_buy_sell_flag'],\n",
    "        )\n",
    "\n",
    "        valid_dataset = lgb.Dataset(\n",
    "            valid_X,\n",
    "            valid_y['target'],\n",
    "            #categorical_feature=['imbalance_buy_sell_flag'],\n",
    "        )\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params=model_params,\n",
    "            train_set=train_dataset,\n",
    "            valid_sets=[train_dataset, valid_dataset],\n",
    "            valid_names=['train', 'valid'],\n",
    "            callbacks=callbacks,\n",
    "            num_boost_round=10000,\n",
    "        )\n",
    "        model.save_model(\n",
    "            outputs_dir.joinpath(f'lightgbm_fold{k+1}.txt'),\n",
    "            num_iteration=model.best_iteration\n",
    "        )\n",
    "        \n",
    "        fimp = model.feature_importance(importance_type='gain')\n",
    "        fimp = pd.DataFrame(fimp, index=feature_columns, columns=[f'fold{k+1}'])\n",
    "        fimps.append(fimp)\n",
    "\n",
    "        train_pred = model.predict(train_X, num_iteration=model.best_iteration)\n",
    "        valid_pred = model.predict(valid_X, num_iteration=model.best_iteration)\n",
    "\n",
    "        history['train_mae'].append(mean_absolute_error(train_y['target'], train_pred))\n",
    "        history['valid_mae'].append(mean_absolute_error(valid_y['target'], valid_pred))\n",
    "\n",
    "        valid_y['regression'] = valid_pred\n",
    "        oofs.append(valid_y)\n",
    "        \n",
    "        del train_X, train_y, train_dataset, valid_X, valid_y, valid_dataset, model, fimp\n",
    "        del train_pred, valid_pred\n",
    "        gc.collect()\n",
    "\n",
    "    history = pd.DataFrame.from_dict(history)\n",
    "    \n",
    "    fimps = pd.concat(fimps, axis=1)\n",
    "    mean_fimps = fimps.mean(axis=1)\n",
    "    std_fimps = fimps.std(axis=1)\n",
    "    fimps['mean_fimps'] = mean_fimps\n",
    "    fimps['std_fimps'] = std_fimps\n",
    "    fimps.sort_values(by='mean_fimps', inplace=True)\n",
    "\n",
    "    oofs = pd.concat(oofs)\n",
    "    oof_mae = mean_absolute_error(oofs['target'], oofs['regression'])\n",
    "    \n",
    "    print(f'test_y mae: {oof_mae:.4f}')\n",
    "    \n",
    "    with open(outputs_dir.joinpath('oofs_lightgbm_optuna.yaml'), 'w') as f:\n",
    "        yaml.dump(\n",
    "            {\n",
    "                'oof_mae': oof_mae,\n",
    "            },\n",
    "            f,\n",
    "            default_flow_style=False\n",
    "        )\n",
    "    return history, oofs, fimps\n",
    "\n",
    "\n",
    "def plot_time(all_time, train_time, valid_time):\n",
    "    _, ax = plt.subplots()\n",
    "    ax.barh(y='all', height=0.6, width=len(all_time), left=all_time.min(), color='tab:blue')\n",
    "    ax.barh(y='train+valid+test', height=0.6, width=[len(train_time), len(valid_time)],\n",
    "            left=[train_time.min(), valid_time.min()], color=['tab:orange', 'tab:green', 'tab:red'])\n",
    "    xcenter = [all_time.min()+len(all_time)//2, train_time.min()+len(train_time)//2,\n",
    "               valid_time.min()+len(valid_time)//2]\n",
    "    ycenter = [0, 1, 1, 1]\n",
    "    width = [f'all\\n{len(all_time)}', f'train\\n{len(train_time)}', f'valid\\n{len(valid_time)}']\n",
    "    for x, y, w in zip(xcenter, ycenter, width):\n",
    "        ax.text(x, y, str(w),  ha='center', va='center')\n",
    "    ax.set_xticks([train_time.min(), valid_time.min(), valid_time.max(), len(all_time)])\n",
    "    ax.grid(axis='x', linestyle='--')\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboosting_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgbdt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     15\u001b[0m }\n\u001b[0;32m---> 17\u001b[0m history, oofs, fimps \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs_dir_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 53\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(df, model_params, outputs_dir, step)\u001b[0m\n\u001b[1;32m     41\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[1;32m     42\u001b[0m     train_X,\n\u001b[1;32m     43\u001b[0m     train_y[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m#categorical_feature=['imbalance_buy_sell_flag'],\u001b[39;00m\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[1;32m     48\u001b[0m     valid_X,\n\u001b[1;32m     49\u001b[0m     valid_y[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m#categorical_feature=['imbalance_buy_sell_flag'],\u001b[39;00m\n\u001b[1;32m     51\u001b[0m )\n\u001b[0;32m---> 53\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m model\u001b[38;5;241m.\u001b[39msave_model(\n\u001b[1;32m     62\u001b[0m     outputs_dir\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightgbm_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     63\u001b[0m     num_iteration\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbest_iteration\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     66\u001b[0m fimp \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfeature_importance(importance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/optiver/lib/python3.10/site-packages/lightgbm/engine.py:276\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    269\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m    270\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    271\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[1;32m    272\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[1;32m    273\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[1;32m    274\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m--> 276\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/optiver/lib/python3.10/site-packages/lightgbm/basic.py:3658\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 3658\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3659\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   3661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'mae',\n",
    "    'metric': 'mae',\n",
    "    'learning_rate': 1e-02,\n",
    "    'seed': 42,\n",
    "    'max_depth':  10,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'feature_fraction': 1.0,\n",
    "    'feature_fraction_bynode': 0.6,\n",
    "    'lambda_l2': 0.0,\n",
    "    #'bagging_fraction': 0.6,\n",
    "    'verbose': -1,\n",
    "}\n",
    "\n",
    "history, oofs, fimps = train(\n",
    "    df=train_df,\n",
    "    model_params=params,\n",
    "    outputs_dir=outputs_dir_path,\n",
    "    step=120,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(oofs)\n",
    "print(fimps.shape)\n",
    "display(fimps.tail(50))\n",
    "\n",
    "_, ax = plt.subplots(figsize=(12, 24))\n",
    "fimps['mean_fimps'].plot(kind='barh', xerr=fimps['std_fimps'], capsize=3, ax=ax)  \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fimps_quantile_th = fimps['mean_fimps'].quantile(q=0.2)\n",
    "display(fimps.query('mean_fimps < @fimps_quantile_th').index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.plot(marker='.', linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.hist2d(oofs['regression'], oofs['target'], bins=100, cmap='Blues', vmax=1e+03)\n",
    "ax.plot([-100, 100], [-100, 100], color='tab:orange')\n",
    "ax.set_xlabel('regression')\n",
    "ax.set_ylabel('target')\n",
    "plt.show()\n",
    "\n",
    "r = np.corrcoef(oofs['regression'], oofs['target'])\n",
    "print(f'correlation coeeficient: {r[0, 1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_per_sotck = oofs.groupby('stock_id')[['target', 'regression']].apply(lambda x: np.mean(abs(x['target'] - x['regression'])))\n",
    "display(mae_per_sotck.describe())\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(mae_per_sotck.values, marker='o', linestyle=':')\n",
    "plt.show()\n",
    "\n",
    "display(mae_per_sotck[mae_per_sotck <= 6])\n",
    "display(mae_per_sotck[mae_per_sotck <= 6].describe())\n",
    "\n",
    "display(mae_per_sotck[mae_per_sotck > 6])\n",
    "display(mae_per_sotck[mae_per_sotck > 6].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(2, 1, sharex=True)\n",
    "bins = np.linspace(-100, 100, 100)\n",
    "axs[0].hist(oofs.query('stock_id in @mae_per_sotck[@mae_per_sotck <= 6].index')['target'], bins=bins, histtype='step', density=True)\n",
    "axs[0].hist(oofs.query('stock_id in @mae_per_sotck[@mae_per_sotck > 6].index')['target'], bins=bins, histtype='step', density=True)\n",
    "axs[1].hist(oofs.query('stock_id in @mae_per_sotck[@mae_per_sotck <= 6].index')['regression'], bins=bins, histtype='step', density=True)\n",
    "axs[1].hist(oofs.query('stock_id in @mae_per_sotck[@mae_per_sotck > 6].index')['regression'], bins=bins, histtype='step', density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(specs=[[{'secondary_y': True}]])\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=oofs.query('stock_id==0').index, y=oofs.query('stock_id==0')['target'],\n",
    "        name='target', mode='lines+markers', marker={'size': 5},\n",
    "    ),\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=oofs.query('stock_id==0').index, y=oofs.query('stock_id==0')['regression'],\n",
    "        name='target', mode='lines+markers', marker={'size': 5},\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(specs=[[{'secondary_y': True}]])\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=oofs.query('stock_id==1').index, y=oofs.query('stock_id==1')['target'],\n",
    "        name='target', mode='lines+markers', marker={'size': 5},\n",
    "    ),\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=oofs.query('stock_id==1').index, y=oofs.query('stock_id==1')['regression'],\n",
    "        name='target', mode='lines+markers', marker={'size': 5},\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train lightgbm model using all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['stock_id', 'date_id', 'seconds_in_bucket', 'target']\n",
    "feature_columns = [col for col in train_df.columns if col not in ['date_id', 'target']]\n",
    "\n",
    "callbacks = [\n",
    "    lgb.log_evaluation(500),\n",
    "]\n",
    "\n",
    "train_dataset = lgb.Dataset(\n",
    "    train_df[feature_columns],\n",
    "    train_df[target_columns]['target'],\n",
    ")\n",
    "\n",
    "del train_df\n",
    "gc.collect()\n",
    "\n",
    "model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=train_dataset,\n",
    "    callbacks=callbacks,\n",
    "    num_boost_round=5000,\n",
    ")\n",
    "\n",
    "model.save_model(\n",
    "    outputs_dir_path.joinpath(f'lightgbm_trained_using_alldata.txt'),\n",
    "    num_iteration=model.best_iteration,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
