{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from itertools import combinations\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "gc.enable()\n",
    "pd.set_option('display.max_columns', 200)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_dir_path = pathlib.Path('../inputs')\n",
    "outputs_dir_path = pathlib.Path('../outputs')\n",
    "if not outputs_dir_path.is_dir():\n",
    "    outputs_dir_path.mkdir()\n",
    "\n",
    "train_df = pd.read_csv(inputs_dir_path. joinpath('train.csv'))\n",
    "train_df.drop(columns=['row_id', 'time_id'], inplace=True)\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_dtypes = {\n",
    "    'stock_id': np.int16,\n",
    "    'date_id': np.int16,\n",
    "    'seconds_in_bucket': np.int16,\n",
    "    'imbalance_size': np.float32,\n",
    "    'imbalance_buy_sell_flag': np.int16,\n",
    "    'reference_price': np.float32,\n",
    "    'matched_size': np.float32,\n",
    "    'far_price': np.float32,\n",
    "    'near_price': np.float32,\n",
    "    'bid_price': np.float32,\n",
    "    'bid_size': np.float32,\n",
    "    'ask_price': np.float32,\n",
    "    'ask_size': np.float32,\n",
    "    'wap': np.float32,\n",
    "    'target': np.float32\n",
    "}\n",
    "\n",
    "display(train_df.dtypes)\n",
    "train_df = train_df.astype(cast_dtypes)\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "])\n",
    "\n",
    "index_wap = (\n",
    "    train_df\n",
    "    .groupby(['date_id', 'seconds_in_bucket'])\n",
    "    .apply(lambda x: (weights[x['stock_id']] * x['wap']).sum() / weights[x['stock_id']].sum())\n",
    ")\n",
    "index_wap = pd.DataFrame(index_wap, columns=['index_wap'])\n",
    "train_df = train_df.merge(index_wap, on=['date_id', 'seconds_in_bucket'])\n",
    "display(train_df)\n",
    "\n",
    "del index_wap\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.set_index(['stock_id', 'date_id', 'seconds_in_bucket']).sort_index().unstack(level=0)\n",
    "train_df.columns = [col[0] + '_' + str(col[1]) for col in train_df.columns]\n",
    "train_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "targets = [f'target_{i}' for i in range(200)]\n",
    "train_df[targets] = train_df[targets].fillna(0)\n",
    "display(train_df[targets])\n",
    "\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train lightgbm models using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        dataset: pd.DataFrame,\n",
    "        model_params: Dict[str, Any],\n",
    "        outputs_dir: pathlib.Path,\n",
    "    ):\n",
    "    \n",
    "    target_columns = ['date_id', 'seconds_in_bucket']\n",
    "    target_columns += [f'target_{i}' for i in range(200)]\n",
    "    feature_columns = [col for col in dataset.columns if col not in target_columns]\n",
    "    days= np.arange(dataset['date_id'].min(), dataset['date_id'].max())\n",
    "    fimps = []\n",
    "    history = {\n",
    "        'train_mae': [],\n",
    "        'valid_mae': [],\n",
    "    }\n",
    "\n",
    "    step = 60\n",
    "    valid_days = days[-step:]\n",
    "    valid_X = dataset.query('date_id in @valid_days')[feature_columns]\n",
    "    valid_y = dataset.query('date_id in @valid_days')[target_columns]\n",
    "\n",
    "    train_day_lower_limits = np.arange(0, 480-step, step)\n",
    "\n",
    "    for k, lower_limit in enumerate(train_day_lower_limits):\n",
    "        train_days = np.arange(lower_limit, lower_limit+step)\n",
    "        print(f'fold {k+1}')\n",
    "        print(train_days)\n",
    "        print(valid_days)\n",
    "        \n",
    "        plot_time(days, train_days, valid_days)\n",
    "        \n",
    "        train_X = dataset.query('date_id in @train_days')[feature_columns]\n",
    "        train_y = dataset.query('date_id in @train_days')[target_columns]\n",
    "        print(f'train_X.shape: {train_X.shape}, train_y.shape: {train_y.shape}')\n",
    "        print(f'valid_X.shape: {valid_X.shape}, valid_y.shape: {valid_y.shape}')\n",
    "        \n",
    "        train_pool = Pool(\n",
    "            data=train_X,\n",
    "            label=train_y[[f'target_{i}' for i in range(10)]],\n",
    "        )\n",
    "\n",
    "        valid_pool = Pool(\n",
    "            data=valid_X,\n",
    "            label=valid_y[[f'target_{i}' for i in range(10)]],\n",
    "        )\n",
    "        \n",
    "        model = CatBoostRegressor(**model_params)\n",
    "        model.fit(\n",
    "            X=train_pool,\n",
    "            eval_set=[(valid_pool)],\n",
    "            use_best_model=True,\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=100,\n",
    "        )\n",
    "        model.save_model(\n",
    "            outputs_dir.joinpath(f'catboost_fold{k+1}.txt'),\n",
    "        )\n",
    "        \n",
    "        fimp = model.get_feature_importance(valid_pool, type='PredictionValuesChange')\n",
    "        fimp = pd.DataFrame(fimp, index=feature_columns, columns=[f'fold{k+1}'])\n",
    "        fimps.append(fimp)\n",
    "\n",
    "        train_pred = model.predict(train_X)\n",
    "        valid_pred = model.predict(valid_X)\n",
    "\n",
    "        valid_y[f'regression_fold{k+1}'] = valid_pred\n",
    "\n",
    "        history['train_mae'].append(mean_absolute_error(train_y['target'], train_pred))\n",
    "        history['valid_mae'].append(mean_absolute_error(valid_y['target'], valid_pred))\n",
    "        \n",
    "        del train_X, train_y, train_pool, valid_pool, model, fimp\n",
    "        del train_pred, valid_pred\n",
    "        gc.collect()\n",
    "\n",
    "    del valid_X\n",
    "    gc.collect()\n",
    "\n",
    "    history = pd.DataFrame.from_dict(history)\n",
    "    \n",
    "    fimps = pd.concat(fimps, axis=1)\n",
    "    mean_fimps = fimps.mean(axis=1)\n",
    "    std_fimps = fimps.std(axis=1)\n",
    "    fimps['mean_fimps'] = mean_fimps\n",
    "    fimps['std_fimps'] = std_fimps\n",
    "    fimps.sort_values(by='mean_fimps', inplace=True)\n",
    "    \n",
    "    valid_y['regression'] = valid_y[[f'regression_fold{k+1}' for k in range(len(train_day_lower_limits))]].mean(axis=1)\n",
    "    test_y_mae = mean_absolute_error(valid_y['target'], valid_y['regression'])\n",
    "    print(f'test_y mae: {test_y_mae:.4f}')\n",
    "    \n",
    "    with open(outputs_dir.joinpath('result_lightgbm_optuna.yaml'), 'w') as f:\n",
    "        yaml.dump(\n",
    "            {\n",
    "                'test_y rmse': test_y_mae,\n",
    "            },\n",
    "            f,\n",
    "            default_flow_style=False\n",
    "        )\n",
    "    return history, valid_y, fimps\n",
    "\n",
    "\n",
    "def plot_time(all_time, train_time, valid_time):\n",
    "    _, ax = plt.subplots()\n",
    "    ax.barh(y='all', height=0.6, width=len(all_time), left=0, color='tab:blue')\n",
    "    ax.barh(y='train+valid+test', height=0.6, width=[len(train_time), len(valid_time)],\n",
    "            left=[train_time.min(), valid_time.min()], color=['tab:orange', 'tab:green', 'tab:red'])\n",
    "    xcenter = [len(all_time)//2, train_time.min()+len(train_time)//2,\n",
    "               valid_time.min()+len(valid_time)//2]\n",
    "    ycenter = [0, 1, 1, 1]\n",
    "    width = [f'all\\n{len(all_time)}', f'train\\n{len(train_time)}', f'valid\\n{len(valid_time)}']\n",
    "    for x, y, w in zip(xcenter, ycenter, width):\n",
    "        ax.text(x, y, str(w),  ha='center', va='center')\n",
    "    ax.set_xticks([train_time.min(), train_time.max(), valid_time.min(), len(all_time)])\n",
    "    ax.grid(axis='x', linestyle='--')\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'loss_function': 'MultiRMSE',\n",
    "    'eval_metric': 'MultiRMSE',\n",
    "    'iterations': 5000,\n",
    "    'depth': 10,\n",
    "    'learning_rate': 5e-03,\n",
    "    'random_state': 42,\n",
    "    'bagging_temperature': 0.8,\n",
    "    'random_strength': 0.8,\n",
    "    #'subsample': 0.8,\n",
    "    'colsample_bylevel': 0.8,\n",
    "    'l2_leaf_reg': 0.0,\n",
    "    'min_data_in_leaf': 20,\n",
    "}\n",
    "\n",
    "history, result, fimps = train(\n",
    "    dataset=train_df,\n",
    "    model_params=params,\n",
    "    outputs_dir=outputs_dir_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result)\n",
    "print(fimps.shape)\n",
    "display(fimps.tail(50))\n",
    "\n",
    "_, ax = plt.subplots(figsize=(12, 36))\n",
    "fimps['mean_fimps'].plot(kind='barh', xerr=fimps['std_fimps'], capsize=3, ax=ax)  \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fimps_quantile_th = fimps['mean_fimps'].quantile(q=0.2)\n",
    "display(fimps.query('mean_fimps < @fimps_quantile_th').index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.plot(marker='.', linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.hist2d(result['regression'], result['target'], bins=100, cmap='Blues', vmax=1e+03)\n",
    "ax.plot([-100, 100], [-100, 100], color='tab:orange')\n",
    "ax.set_xlabel('regression')\n",
    "ax.set_ylabel('target')\n",
    "plt.show()\n",
    "\n",
    "r = np.corrcoef(result['regression'], result['target'])\n",
    "print(f'correlation coeeficient: {r[0, 1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train lightgbm model using all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['stock_id', 'date_id', 'seconds_in_bucket', 'target']\n",
    "feature_columns = [col for col in train_df.columns if col not in ['date_id', 'target']]\n",
    "\n",
    "train_pool = Pool(\n",
    "    data=train_df[feature_columns],\n",
    "    label=train_df[target_columns]['target'],\n",
    ")\n",
    "\n",
    "del train_df\n",
    "gc.collect()\n",
    "\n",
    "model = CatBoostRegressor(**params)\n",
    "model.fit(train_pool, use_best_model=True, verbose=100)\n",
    "\n",
    "model.save_model(\n",
    "    outputs_dir_path.joinpath(f'catboost_trained_using_alldata.txt'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
