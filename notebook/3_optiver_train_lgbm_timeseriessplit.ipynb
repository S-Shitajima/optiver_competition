{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from itertools import combinations\n",
    "import pathlib\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "gc.enable()\n",
    "pd.set_option('display.max_columns', 200)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame):\n",
    "    df = pl.from_pandas(df)\n",
    "    new_features1 = [\n",
    "        (pl.col('date_id') % 5).alias('day_of_week'),\n",
    "        (pl.col('imbalance_buy_sell_flag') + 1).alias('imbalance_buy_sell_flag'),\n",
    "        (pl.col('ask_price') - pl.col('bid_price')).alias('diff_ask_price_bid_price'),\n",
    "        (pl.col('ask_price') - pl.col('reference_price')).alias('diff_ask_price_reference_price'),\n",
    "        (pl.col('bid_price') - pl.col('reference_price')).alias('diff_bid_price_reference_price'),\n",
    "        (pl.col('ask_price') - pl.col('wap')).alias('diff_ask_price_wap'),\n",
    "        (pl.col('bid_price') - pl.col('wap')).alias('diff_bid_price_wap'),\n",
    "        (pl.col('far_price') - pl.col('near_price')).alias('diff_far_price_near_price'),\n",
    "        (pl.col('far_price') - pl.col('reference_price')).alias('diff_far_price_reference_price'),\n",
    "        (pl.col('near_price') - pl.col('reference_price')).alias('diff_near_price_reference_price'),\n",
    "        (pl.col('ask_size') - pl.col('bid_size')).alias('diff_ask_size_bid_size'),\n",
    "        (pl.col('ask_size') - pl.col('matched_size')).alias('diff_ask_size_matched_size'),\n",
    "        (pl.col('bid_size') - pl.col('matched_size')).alias('diff_bid_size_matched_size'),\n",
    "        (pl.col('imbalance_size') - pl.col('matched_size')).alias('diff_imbalance_size_matched_size'),\n",
    "        (pl.col('ask_price') + pl.col('bid_price')).alias('sum_ask_price_bid_price'),\n",
    "        (pl.col('far_price') + pl.col('near_price')).alias('sum_far_price_near_price'),\n",
    "        (pl.col('ask_size') + pl.col('bid_size')).alias('sum_ask_size_bid_size'),\n",
    "        (pl.col('bid_size') * pl.col('ask_price') / (pl.col('bid_size') + pl.col('ask_size'))).alias('wap_factor1'),\n",
    "        (pl.col('ask_size') * pl.col('bid_price') / (pl.col('bid_size') + pl.col('ask_size'))).alias('wap_factor2'),\n",
    "        (pl.col('near_price') / pl.col('far_price')).alias('div_near_price_far_price'),\n",
    "\n",
    "        ((pl.col('ask_price') - pl.col('bid_price')) / (pl.col('reference_price') - pl.col('bid_price'))).alias('feature1'), \n",
    "        ((pl.col('ask_price') - pl.col('bid_price')) / (pl.col('wap') - pl.col('bid_price'))).alias('feature2'), \n",
    "        ((pl.col('ask_price') - pl.col('reference_price')) / (pl.col('bid_price') - pl.col('reference_price'))).alias('feature3'), \n",
    "        ((pl.col('ask_price') - pl.col('reference_price')) / (pl.col('wap') - pl.col('reference_price'))).alias('feature4'), \n",
    "        ((pl.col('ask_price') - pl.col('wap')) / (pl.col('bid_price') - pl.col('wap'))).alias('feature5'), \n",
    "        ((pl.col('ask_price') - pl.col('wap')) / (pl.col('reference_price') - pl.col('wap'))).alias('feature6'),\n",
    "        ((pl.col('bid_price') - pl.col('ask_price')) / (pl.col('reference_price') - pl.col('ask_price'))).alias('feature7'),\n",
    "        ((pl.col('bid_price') - pl.col('ask_price')) / (pl.col('wap') - pl.col('ask_price'))).alias('feature8'),\n",
    "        ((pl.col('bid_price') - pl.col('reference_price')) / (pl.col('wap') - pl.col('reference_price'))).alias('feature9'),\n",
    "        ((pl.col('bid_price') - pl.col('wap')) / (pl.col('reference_price') - pl.col('wap'))).alias('feature10'),\n",
    "        ((pl.col('reference_price') - pl.col('ask_price')) / (pl.col('wap') - pl.col('ask_price'))).alias('feature11'),\n",
    "        ((pl.col('reference_price') - pl.col('bid_price')) / (pl.col('wap') - pl.col('bid_price'))).alias('feature12'),\n",
    "\n",
    "        ((pl.col('ask_size') - pl.col('bid_size')) / (pl.col('matched_size') - pl.col('bid_size'))).alias('feature13'), \n",
    "        ((pl.col('ask_size') - pl.col('bid_size')) / (pl.col('imbalance_size') - pl.col('bid_size'))).alias('feature14'), \n",
    "        ((pl.col('ask_size') - pl.col('matched_size')) / (pl.col('bid_size') - pl.col('imbalance_size'))).alias('feature15'), \n",
    "        ((pl.col('ask_size') - pl.col('matched_size')) / (pl.col('imbalance_size') - pl.col('matched_size'))).alias('feature16'), \n",
    "        ((pl.col('ask_size') - pl.col('imbalance_size')) / (pl.col('bid_size') - pl.col('imbalance_size'))).alias('feature17'), \n",
    "        ((pl.col('ask_size') - pl.col('imbalance_size')) / (pl.col('matched_size') - pl.col('imbalance_size'))).alias('feature18'),\n",
    "        ((pl.col('bid_size') - pl.col('ask_size')) / (pl.col('matched_size') - pl.col('ask_size'))).alias('feature19'),\n",
    "        ((pl.col('bid_size') - pl.col('ask_size')) / (pl.col('imbalance_size') - pl.col('ask_size'))).alias('feature20'),\n",
    "        ((pl.col('bid_size') - pl.col('matched_size')) / (pl.col('imbalance_size') - pl.col('matched_size'))).alias('feature21'),\n",
    "        ((pl.col('bid_size') - pl.col('imbalance_size')) / (pl.col('matched_size') - pl.col('imbalance_size'))).alias('feature22'),\n",
    "        ((pl.col('matched_size') - pl.col('ask_size')) / (pl.col('imbalance_size') - pl.col('ask_size'))).alias('feature23'),\n",
    "        ((pl.col('matched_size') - pl.col('bid_size')) / (pl.col('imbalance_size') - pl.col('bid_size'))).alias('feature24'),\n",
    "\n",
    "        (pl.col('bid_price') / pl.col('ask_price')).alias('feature25'),\n",
    "        (pl.col('reference_price') / pl.col('ask_price')).alias('feature26'),\n",
    "        (pl.col('wap') / pl.col('ask_price')).alias('feature27'),\n",
    "        (pl.col('reference_price') / pl.col('bid_price')).alias('feature28'),\n",
    "        (pl.col('wap') / pl.col('bid_price')).alias('feature29'),\n",
    "\n",
    "        (pl.col('bid_size') / pl.col('ask_size')).alias('feature30'),\n",
    "        (pl.col('ask_size') / pl.col('matched_size')).alias('feature31'),\n",
    "        (pl.col('bid_size') / pl.col('matched_size')).alias('feature32'),\n",
    "        (pl.col('imbalance_size') / pl.col('matched_size')).alias('feature33'),\n",
    "        (pl.col('ask_size') / pl.col('imbalance_size')).alias('feature34'),\n",
    "        (pl.col('bid_size') / pl.col('imbalance_size')).alias('feature35'),\n",
    "\n",
    "        (pl.col('imbalance_size') * (pl.col('ask_price') - pl.col('bid_price'))).alias('feature36'),\n",
    "        ((pl.col('ask_price') - pl.col('bid_price')) * ((pl.col('ask_size') - pl.col('bid_size')) / (pl.col('ask_size') + pl.col('bid_size')))).alias('feature37'),\n",
    "    ]\n",
    "\n",
    "    new_features2 = [\n",
    "        pl.col('wap').mean().over(['date_id', 'seconds_in_bucket']).alias(f'mean_wap_over_stocks'),\n",
    "    ]\n",
    "\n",
    "    lag_features = [\n",
    "        'ask_price',\n",
    "        'bid_price',\n",
    "        'reference_price',\n",
    "        'wap',\n",
    "        'far_price',\n",
    "        'near_price',\n",
    "        'mean_wap_over_stocks',\n",
    "        'ask_size',\n",
    "        'bid_size',\n",
    "        'matched_size',\n",
    "        'imbalance_size',\n",
    "    ]\n",
    "\n",
    "    new_features3 = [\n",
    "        # pl.col(lag_features).diff(n=1).over(['stock_id']).name.suffix('_diff1'),\n",
    "        # pl.col(lag_features).diff(n=2).over(['stock_id']).name.suffix('_diff2'),\n",
    "        # pl.col(lag_features).diff(n=3).over(['stock_id']).name.suffix('_diff3'),\n",
    "        # pl.col(lag_features).diff(n=4).over(['stock_id']).name.suffix('_diff4'),\n",
    "        # pl.col(lag_features).diff(n=5).over(['stock_id']).name.suffix('_diff5'),\n",
    "        # pl.col(lag_features).diff(n=6).over(['stock_id']).name.suffix('_diff6'),\n",
    "\n",
    "        pl.col(lag_features).pct_change(n=1).over(['stock_id']).name.suffix('pct_change1'),\n",
    "        pl.col(lag_features).pct_change(n=2).over(['stock_id']).name.suffix('pct_change2'),\n",
    "        pl.col(lag_features).pct_change(n=3).over(['stock_id']).name.suffix('pct_change3'),\n",
    "        pl.col(lag_features).pct_change(n=4).over(['stock_id']).name.suffix('pct_change4'),\n",
    "        pl.col(lag_features).pct_change(n=5).over(['stock_id']).name.suffix('pct_change5'),\n",
    "        pl.col(lag_features).pct_change(n=6).over(['stock_id']).name.suffix('pct_change6'),\n",
    "    ]\n",
    "    \n",
    "    transform_features = [\n",
    "        'imbalance_size',\n",
    "        'reference_price',\n",
    "        'matched_size',\n",
    "        'far_price',\n",
    "        'near_price',\n",
    "        'bid_price',\n",
    "        'bid_size',\n",
    "        'ask_price',\n",
    "        'ask_size',\n",
    "        'wap',\n",
    "    ]\n",
    "\n",
    "    new_features4 = [\n",
    "        ((pl.col(feature) - pl.col(feature).quantile(0.5)) / (pl.col(feature).quantile(0.75) - pl.col(feature).quantile(0.25)))\n",
    "        .over(['date_id', 'seconds_in_bucket'])\n",
    "        .alias(f'robust_scaled_{feature}_over_stocks')\n",
    "        for feature in transform_features\n",
    "    ]\n",
    "    \n",
    "    df = (\n",
    "        df\n",
    "        .with_columns(new_features1)\n",
    "        .with_columns(new_features2)\n",
    "        .with_columns(new_features3)\n",
    "        .drop(['row_id'])\n",
    "        .to_pandas()gra\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_dir_path = pathlib.Path('../inputs')\n",
    "outputs_dir_path = pathlib.Path('../outputs')\n",
    "if not outputs_dir_path.is_dir():\n",
    "    outputs_dir_path.mkdir()\n",
    "\n",
    "train_df = pd.read_csv(inputs_dir_path. joinpath('train.csv'))\n",
    "train_df.drop(columns=['time_id'], inplace=True)\n",
    "train_df = preprocess(train_df)\n",
    "train_df = train_df.dropna(subset=['target'])\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train lightgbm models using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        dataset: pd.DataFrame,\n",
    "        outputs_dir: pathlib.Path,\n",
    "    ):\n",
    "    \n",
    "    target_columns = ['stock_id', 'date_id', 'seconds_in_bucket', 'target']\n",
    "    feature_columns = [col for col in dataset.columns if col not in ['date_id', 'target']]\n",
    "    days= np.arange(dataset['date_id'].min(), dataset['date_id'].max())\n",
    "    fimps = []\n",
    "    best_param_records = {}\n",
    "    best_value_records = {}\n",
    "    history = {\n",
    "        'train_mae': [],\n",
    "        'valid_mae': [],\n",
    "        'test_mae': [],\n",
    "    }\n",
    "    valid_days = days[-60*2:-60]\n",
    "    test_days = days[-60:]\n",
    "    valid_X = dataset.query('date_id in @valid_days')[feature_columns]\n",
    "    valid_y = dataset.query('date_id in @valid_days')[target_columns]\n",
    "    test_X = dataset.query('date_id in @test_days')[feature_columns]\n",
    "    test_y = dataset.query('date_id in @test_days')[target_columns]\n",
    "\n",
    "    train_day_limits = [(300, 360), (240, 360), (180, 360), (120, 360), (60, 360), (0, 360)]\n",
    "\n",
    "    for k, limits in enumerate(train_day_limits):\n",
    "        train_days = np.arange(*limits)\n",
    "        print(f'fold {k+1}')\n",
    "        print(train_days)\n",
    "        print(valid_days)\n",
    "        \n",
    "        plot_time(days, train_days, valid_days, test_days)\n",
    "        \n",
    "        train_X = dataset.query('date_id in @train_days')[feature_columns]\n",
    "        train_y = dataset.query('date_id in @train_days')[target_columns]\n",
    "        print(f'train_X.shape: {train_X.shape}, train_y.shape: {train_y.shape}')\n",
    "        print(f'valid_X.shape: {valid_X.shape}, valid_y.shape: {valid_y.shape}')\n",
    "        print(f'test_X.shape: {test_X.shape}, test_y.shape: {test_y.shape}')\n",
    "        \n",
    "        objective = Objective(train_X, train_y['target'], valid_X, valid_y['target'])\n",
    "        study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=42))\n",
    "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        study.optimize(objective, n_trials=5)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        add_params = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'mae',\n",
    "            'metric': 'mae',\n",
    "            'seed': 42,\n",
    "            'num_leaves': 256,\n",
    "        }\n",
    "        best_params.update(add_params)\n",
    "        \n",
    "        [print(f'{k}: {v}') for k, v in best_params.items()]\n",
    "        print(f'best value: {study.best_value}')\n",
    "\n",
    "        callbacks = [\n",
    "            lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
    "            lgb.log_evaluation(100),\n",
    "        ]\n",
    "        \n",
    "        train_dataset = lgb.Dataset(\n",
    "            train_X,\n",
    "            train_y['target'],\n",
    "            #categorical_feature=['imbalance_buy_sell_flag'],\n",
    "        )\n",
    "\n",
    "        valid_dataset = lgb.Dataset(\n",
    "            valid_X,\n",
    "            valid_y['target'],\n",
    "            #categorical_feature=['imbalance_buy_sell_flag'],\n",
    "        )\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params=best_params,\n",
    "            train_set=train_dataset,\n",
    "            valid_sets=[train_dataset, valid_dataset],\n",
    "            valid_names=['train', 'valid'],\n",
    "            callbacks=callbacks,\n",
    "            num_boost_round=3000,\n",
    "        )\n",
    "        model.save_model(\n",
    "            outputs_dir.joinpath(f'lightgbm_optuna_fold{k+1}.txt'),\n",
    "            num_iteration=model.best_iteration\n",
    "        )\n",
    "        \n",
    "        best_params['num_boost_round'] = model.best_iteration\n",
    "        best_param_records[f'fold{k+1}'] = best_params\n",
    "        best_value_records[f'fold{k+1}'] = study.best_value\n",
    "\n",
    "        fimp = model.feature_importance(importance_type='gain')\n",
    "        fimp = pd.DataFrame(fimp, index=feature_columns, columns=[f'fold{k+1}'])\n",
    "        fimps.append(fimp)\n",
    "\n",
    "        train_pred = model.predict(train_X, num_iteration=model.best_iteration)\n",
    "        valid_pred = model.predict(valid_X, num_iteration=model.best_iteration)\n",
    "        test_pred = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "\n",
    "        test_y[f'regression_fold{k+1}'] = test_pred\n",
    "\n",
    "        history['train_mae'].append(mean_absolute_error(train_y['target'], train_pred))\n",
    "        history['valid_mae'].append(mean_absolute_error(valid_y['target'], valid_pred))\n",
    "        history['test_mae'].append(mean_absolute_error(test_y['target'], test_pred))\n",
    "        \n",
    "        del objective, study\n",
    "        del train_X, train_y, train_dataset, valid_dataset, model, fimp\n",
    "        del train_pred, valid_pred, test_pred\n",
    "        gc.collect()\n",
    "\n",
    "    del valid_X, valid_y, test_X\n",
    "    gc.collect()\n",
    "\n",
    "    history = pd.DataFrame.from_dict(history)\n",
    "    \n",
    "    fimps = pd.concat(fimps, axis=1)\n",
    "    mean_fimps = fimps.mean(axis=1)\n",
    "    std_fimps = fimps.std(axis=1)\n",
    "    fimps['mean_fimps'] = mean_fimps\n",
    "    fimps['std_fimps'] = std_fimps\n",
    "    fimps.sort_values(by='mean_fimps', inplace=True)\n",
    "    \n",
    "    test_y['regression'] = test_y[[f'regression_fold{k+1}' for k in range(len(train_day_limits))]].mean(axis=1)\n",
    "    test_y_mae = mean_absolute_error(test_y['target'], test_y['regression'])\n",
    "    print(f'test_y mae: {test_y_mae:.4f}')\n",
    "    \n",
    "    with open(outputs_dir.joinpath('result_lightgbm_optuna.yaml'), 'w') as f:\n",
    "        yaml.dump(\n",
    "            {\n",
    "                'best_param_records': best_param_records,\n",
    "                'best_value_records': best_value_records,\n",
    "                'test_y rmse': test_y_mae,\n",
    "            },\n",
    "            f,\n",
    "            default_flow_style=False\n",
    "        )\n",
    "    return history, test_y, fimps, best_param_records\n",
    "\n",
    "\n",
    "class Objective:\n",
    "    def __init__(self, train_X, train_y, valid_X, valid_y):\n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.valid_X = valid_X\n",
    "        self.valid_y = valid_y\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        params = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'mae',\n",
    "            'metric': 'mae',\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-03, 1e-01),\n",
    "            'seed': 42,\n",
    "            'max_depth':  trial.suggest_int('max_depth', 3, 10),\n",
    "            'num_leaves': 256,\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 100),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction_bynode', 0.4, 1.0),\n",
    "            'feature_fraction_bynode': trial.suggest_float('feature_fraction_bynode', 0.4, 1.0),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 0.1, 10),\n",
    "            #'bagging_fraction': 0.6,\n",
    "            'verbose': -1,\n",
    "        }\n",
    "        \n",
    "        callbacks = [\n",
    "            lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
    "            lgb.log_evaluation(0),\n",
    "        ]\n",
    "        \n",
    "        train_dataset = lgb.Dataset(self.train_X, self.train_y)\n",
    "        valid_dataset = lgb.Dataset(self.valid_X, self.valid_y)\n",
    "\n",
    "        model = lgb.train(\n",
    "            params=params,\n",
    "            train_set=train_dataset,\n",
    "            valid_sets=[train_dataset, valid_dataset],\n",
    "            valid_names=['train', 'valid'],\n",
    "            callbacks=callbacks,\n",
    "            num_boost_round=3000,\n",
    "        )\n",
    "\n",
    "        preds = model.predict(self.valid_X, num_iteration=model.best_iteration)\n",
    "        mae = mean_absolute_error(preds, self.valid_y)\n",
    "\n",
    "        del params, callbacks, train_dataset, valid_dataset, model, preds\n",
    "        gc.collect()\n",
    "\n",
    "        return mae\n",
    "    \n",
    "\n",
    "def plot_time(all_time, train_time, valid_time, test_time):\n",
    "    _, ax = plt.subplots()\n",
    "    ax.barh(y='all', height=0.6, width=len(all_time), left=0, color='tab:blue')\n",
    "    ax.barh(y='train+valid+test', height=0.6, width=[len(train_time), len(valid_time), len(test_time)],\n",
    "            left=[train_time.min(), valid_time.min(), test_time.min()], color=['tab:orange', 'tab:green', 'tab:red'])\n",
    "    xcenter = [len(all_time)//2, train_time.min()+len(train_time)//2,\n",
    "               valid_time.min()+len(valid_time)//2, test_time.min()+len(test_time)//2]\n",
    "    ycenter = [0, 1, 1, 1]\n",
    "    width = [f'all\\n{len(all_time)}', f'train\\n{len(train_time)}', f'valid\\n{len(valid_time)}', f'test\\n{len(test_time)}']\n",
    "    for x, y, w in zip(xcenter, ycenter, width):\n",
    "        ax.text(x, y, str(w),  ha='center', va='center')\n",
    "    ax.set_xticks([train_time.min(), valid_time.min(), test_time.min(), len(all_time)])\n",
    "    ax.grid(axis='x', linestyle='--')\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history, result, fimps, best_params_records = train(\n",
    "    dataset=train_df,\n",
    "    outputs_dir=outputs_dir_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result)\n",
    "print(fimps.shape)\n",
    "display(fimps.tail(50))\n",
    "\n",
    "_, ax = plt.subplots(figsize=(12, 18))\n",
    "fimps['mean_fimps'].plot(kind='barh', xerr=fimps['std_fimps'], capsize=3, ax=ax)  \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fimps_quantile_th = fimps['mean_fimps'].quantile(q=0.2)\n",
    "display(fimps.query('mean_fimps < @fimps_quantile_th').index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.plot(marker='.', linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.hist2d(result['regression'], result['target'], bins=100, cmap='Blues', vmax=1e+03)\n",
    "ax.plot([-100, 100], [-100, 100], color='tab:orange')\n",
    "ax.set_xlabel('regression')\n",
    "ax.set_ylabel('target')\n",
    "plt.show()\n",
    "\n",
    "r = np.corrcoef(result['regression'], result['target'])\n",
    "print(f'correlation coeeficient: {r[0, 1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train lightgbm model using all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = pd.DataFrame(best_params_records)\n",
    "display(best_params)\n",
    "\n",
    "new_best_params = {}\n",
    "for idx in best_params.index:\n",
    "    if best_params.loc[idx].nunique() > 1:\n",
    "        new_best_params[idx] = best_params.loc[idx].mean().item()\n",
    "    else:\n",
    "        new_best_params[idx] = best_params.loc[idx].unique().item()\n",
    "    if isinstance(best_params.loc[idx].iloc[0], int):\n",
    "        new_best_params[idx] = int(new_best_params[idx])\n",
    "\n",
    "num_boost_round = new_best_params.pop('num_boost_round')\n",
    "print(new_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['stock_id', 'date_id', 'seconds_in_bucket', 'target']\n",
    "feature_columns = [col for col in train_df.columns if col not in ['date_id', 'target']]\n",
    "\n",
    "callbacks = [\n",
    "    lgb.log_evaluation(100),\n",
    "]\n",
    "\n",
    "train_dataset = lgb.Dataset(\n",
    "    train_df[feature_columns],\n",
    "    train_df[target_columns]['target'],\n",
    ")\n",
    "\n",
    "del train_df\n",
    "gc.collect()\n",
    "\n",
    "model = lgb.train(\n",
    "    params=new_best_params,\n",
    "    train_set=train_dataset,\n",
    "    callbacks=callbacks,\n",
    "    num_boost_round=num_boost_round,\n",
    ")\n",
    "\n",
    "model.save_model(\n",
    "    outputs_dir_path.joinpath(f'lightgbm_trained_using_alldata.txt'),\n",
    "    num_iteration=model.best_iteration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optiver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
